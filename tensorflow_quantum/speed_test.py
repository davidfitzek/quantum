# -*- coding: utf-8 -*-
"""Copy of New Full Experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wlVIDKypMvqk5UDULHafXu8aBgqf0jez
"""

import tensorflow as tf
# import tensorflow_quantum as tfq
from tensorflow_quantum.python import util
from tensorflow_quantum.core.ops.math_ops import inner_product_op
from tensorflow_quantum.core.ops import tfq_utility_ops
from tensorflow_quantum.python.layers.circuit_executors import expectation
from tensorflow_quantum.python.layers.circuit_construction import elementary

import cirq
import numpy as np
import numexpr as ne
import sympy
import pickle
import pandas as pd
import seaborn as sns
sns.set(style="white")
import matplotlib as mpl
import matplotlib.pyplot as plt
from scipy.sparse.linalg import eigs
from sklearn.svm import SVC, SVR
from sklearn.model_selection import GridSearchCV, PredefinedSplit
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.decomposition import PCA
import multiprocessing
from multiprocessing import Pool
import time
from sklearn.metrics.pairwise import rbf_kernel

cpu_cnt = multiprocessing.cpu_count()
print(cpu_cnt)
# tfq.set_quantum_concurrent_op_mode(True)
"""Helper methods."""


def generate_fashion_mnist(feature_number, data_n, feature_range=1.0):
    pca_components = feature_number
    (x_init, y_init), _ = tf.keras.datasets.fashion_mnist.load_data()

    # Rescale the images from [0,255] to the [0.0,1.0] range.
    x_init = x_init[..., np.newaxis] / 255.0
    print("Number of original training examples:", len(x_init))

    def filter_binary_class(x, y):
        keep = (y == 0) | (y == 3)
        x, y = x[keep], y[keep]
        y = (y == 0)
        return x, y

    x_init, y_init = filter_binary_class(x_init, y_init)

    print("Number of filtered training examples:", len(x_init))

    x_init = x_init.reshape(len(x_init), -1)

    pca = PCA(n_components=pca_components)
    pca.fit(x_init)
    x_init = pca.transform(x_init)

    for k in range(pca_components):
        s = np.std(x_init[:, k]) / feature_range
        #s = max(np.max(x_init[:, k]), -np.min(x_init[:, k])) / feature_range
        x_init[:, k] /= s
        print("Range of feature", k, np.min(x_init[:, k]), np.max(x_init[:, k]),
              np.std(x_init[:, k]))

    perm_idx = np.random.permutation(len(x_init))
    x_init = np.array([x_init[i] for i in perm_idx])[:data_n]
    y_init = np.array([y_init[i] for i in perm_idx])[:data_n]

    return x_init, y_init


def generate_data_batch(qubits, classical_source, trotter=5, option=0):
    """Encodes data in the following way:
  
  1. Randomly rotates all qubits.
  2. Apply e^-i \alpha[i][j] XX + YY + ZZ (in 1D) where alpha[i][j] are given by
      `classical_source`

  Args:
    qubits: Python `lst` of `cirq.GridQubits`. The qubits on which we will
      generate this data batch.
    classical_source: `np.ndarray` of shape [n_points, len(qubits) - 1] giving
      the coefficients for use in the exponential above.
    trotter: Scalar giving the number of Trotter steps

  Returns:
    `tf.Tensor` where the circuit at index i corresponds to a circuit realization
      from step 2.
  """
    n_qubits = len(qubits)
    n_points = len(classical_source)

    QNN = cirq.Circuit()
    random_Js = np.random.normal(0, 1.0, size=(n_qubits - 1))
    for step in range(5):
        for i, (q0, q1) in enumerate(zip(qubits, qubits[1:])):
            QNN += cirq.XX(q0, q1)**random_Js[i]
            QNN += cirq.YY(q0, q1)**random_Js[i]
            QNN += cirq.ZZ(q0, q1)**random_Js[i]

    if option == 0:  # Hamiltonian simulation
        initial_psi = cirq.Circuit()
        random_rots = np.random.uniform(-2, 2, size=(n_qubits, 3))
        for i, qubit in enumerate(qubits):
            initial_psi.append(cirq.X(qubit)**random_rots[i][0])
            initial_psi.append(cirq.Y(qubit)**random_rots[i][1])
            initial_psi.append(cirq.Z(qubit)**random_rots[i][2])

        ref_paulis = [
            cirq.X(q0) * cirq.X(q1) + \
            cirq.Y(q0) * cirq.Y(q1) + \
            cirq.Z(q0) * cirq.Z(q1) for q0, q1 in zip(qubits, qubits[1:])
        ]
        exp_symbols = list(sympy.symbols('ref_0:' + str(len(ref_paulis))))
        exp_circuit = util.exponential(ref_paulis, exp_symbols)
        for t in range(trotter - 1):
            exp_circuit += util.exponential(ref_paulis, exp_symbols)
            # cirq.MergeInteractions().optimize_circuit(exp_circuit)
            # cirq.DropEmptyMoments().optimize_circuit(exp_circuit)
            print('SIZE', len(exp_circuit))

        initial_psi_tensor = util.convert_to_tensor([initial_psi])
        initial_psi_splat = tf.tile(initial_psi_tensor, [n_points])

        full_circuits = elementary.AddCircuit()(initial_psi_splat,
                                                append=exp_circuit)
        full_QNN_circuits = elementary.AddCircuit()(initial_psi_splat,
                                                    append=exp_circuit + QNN)

        return tfq_utility_ops.resolve_parameters(full_circuits,
                                      tf.convert_to_tensor([str(x) for x in exp_symbols]),
                                      tf.convert_to_tensor(classical_source*(n_qubits/3)/trotter)),\
               tfq_utility_ops.resolve_parameters(full_QNN_circuits,
                                      tf.convert_to_tensor([str(x) for x in exp_symbols]),
                                      tf.convert_to_tensor(classical_source*(n_qubits/3)/trotter))
    if option == 1:  # Qubit rotation
        initial_psi = cirq.Circuit()
        for i, qubit in enumerate(qubits):
            initial_psi.append(cirq.X(qubit)**0)
            initial_psi.append(cirq.Y(qubit)**0)
            initial_psi.append(cirq.Z(qubit)**0)

        ref_paulis = [cirq.X(q0) for q0 in qubits[:-1]]
        exp_symbols = list(sympy.symbols('ref_0:' + str(len(ref_paulis))))
        exp_circuit = util.exponential(ref_paulis, exp_symbols)

        initial_psi_tensor = convert_to_tensor([initial_psi])
        initial_psi_splat = tf.tile(initial_psi_tensor, [n_points])

        full_circuits = elementary.AddCircuit()(initial_psi_splat,
                                                append=exp_circuit)
        full_QNN_circuits = elementary.AddCircuit()(initial_psi_splat,
                                                    append=exp_circuit + QNN)

        return tfq_utility_ops.resolve_parameters(full_circuits,
                                      tf.convert_to_tensor([str(x) for x in exp_symbols]),
                                      tf.convert_to_tensor(classical_source / np.pi)),\
               tfq_utility_ops.resolve_parameters(full_QNN_circuits,
                                      tf.convert_to_tensor([str(x) for x in exp_symbols]),
                                      tf.convert_to_tensor(classical_source / np.pi))
    if option == 2:  # IQP
        initial_psi = cirq.Circuit()
        for i, qubit in enumerate(qubits):
            initial_psi.append(cirq.X(qubit)**0)
            initial_psi.append(cirq.Y(qubit)**0)
            initial_psi.append(cirq.Z(qubit)**0)

        ref_paulis = [cirq.Z(q0) for q0 in qubits[:-1]]\
         + [cirq.Z(q0) * cirq.Z(q1) for i, q0 in enumerate(qubits[:-1]) for j, q1 in enumerate(qubits[:-1]) if i < j]
        exp_symbols = list(sympy.symbols('ref_0:' + str(len(ref_paulis))))
        exp_circuit = util.exponential(ref_paulis, exp_symbols)

        later_circuit = cirq.Circuit()
        for i, qubit in enumerate(qubits):
            later_circuit += cirq.H(qubit)
        later_circuit += exp_circuit
        for i, qubit in enumerate(qubits):
            later_circuit += cirq.H(qubit)
        later_circuit += exp_circuit

        initial_psi_tensor = util.convert_to_tensor([initial_psi])
        initial_psi_splat = tf.tile(initial_psi_tensor, [n_points])

        full_circuits = elementary.AddCircuit()(initial_psi_splat,
                                                append=later_circuit)
        full_QNN_circuits = elementary.AddCircuit()(initial_psi_splat,
                                                    append=later_circuit + QNN)

        # Transform classical-source
        classical_source_new = np.zeros(
            (n_points, n_qubits - 1 + (n_qubits - 2) * (n_qubits - 1) // 2))
        for i in range(n_points):
            cnt = 0
            for p in range(n_qubits - 1):
                classical_source_new[i, cnt] = classical_source[i, p] / np.pi
                cnt += 1
            for p in range(n_qubits - 1):
                for q in range(p + 1, n_qubits - 1):
                    classical_source_new[i, cnt] = classical_source[
                        i, p] * classical_source[i, q] / np.pi
                    cnt += 1

        return tfq_utility_ops.resolve_parameters(full_circuits,
                                      tf.convert_to_tensor([str(x) for x in exp_symbols]),
                                      tf.convert_to_tensor(classical_source_new)),\
               tfq_utility_ops.resolve_parameters(full_QNN_circuits,
                                      tf.convert_to_tensor([str(x) for x in exp_symbols]),
                                      tf.convert_to_tensor(classical_source_new))


def get_one_RDM_batches(qubits, data_batch):
    """Get RDM data in the following form:

  Returns rdm[i][j][k] where i gives the batch index,
    j gives the qubit index and k = 0 gives <X>, k= 1 gives <Y> and k = 2 gives <Z>.
  """
    ops = [[cirq.X(q), cirq.Y(q), cirq.Z(q)] for q in qubits]
    ops_tensor = tf.expand_dims(tf.reshape(util.convert_to_tensor(ops), -1), 0)
    batch_dim = tf.gather(tf.shape(data_batch), 0)
    ops_splat = tf.tile(ops_tensor, [batch_dim, 1])
    exp_vals = expectation.Expectation()(data_batch, operators=ops_splat)
    return tf.reshape(exp_vals, [batch_dim, len(qubits), -1])


def get_QNN_batches(qubits, data_batch):
    """Get RDM data in the following form:

  Returns rdm[i][j][k] where i gives the batch index,
    j gives the qubit index and k = 0 gives <X>, k= 1 gives <Y> and k = 2 gives <Z>.
  """
    ops = [cirq.Z(qubits[1])]
    ops_tensor = tf.expand_dims(tf.reshape(util.convert_to_tensor(ops), -1), 0)
    batch_dim = tf.gather(tf.shape(data_batch), 0)
    ops_splat = tf.tile(ops_tensor, [batch_dim, 1])
    exp_vals = expectation.Expectation()(data_batch, operators=ops_splat)
    return tf.reshape(exp_vals, [batch_dim, -1])


def get_rdm_dataset(qubits, x_init, trotter=5, option=0):
    big_data_tensor, big_data_tensor_QNN = generate_data_batch(qubits,
                                                               x_init,
                                                               trotter=trotter,
                                                               option=option)
    rdms = get_one_RDM_batches(qubits, big_data_tensor)
    y_QNN = get_QNN_batches(qubits, big_data_tensor_QNN).numpy().ravel()

    #y_QNN = np.array([1 if y > np.median(y_QNN) else -1 for y in y_QNN])
    #y_QNN = np.array([y if np.random.uniform() < 0.95 else -y for y in y_QNN])

    # Reshape rdms to vectors. [batch, n_qubits * 3]
    return big_data_tensor, tf.reshape(
        rdms, [tf.gather(tf.shape(rdms), 0), -1]), y_QNN


def compute_kernel_matrix(vecs, gamma):
    """Computes d[i][j] = e^ -gamma * (vecs[i] - vecs[j]) ** 2 """
    scaled_gamma = gamma / (vecs.shape[1] * tf.math.reduce_std(vecs))
    X = vecs.numpy()
    X_norm = np.sum(X**2, axis=-1)
    K = ne.evaluate(
        'exp(-g * (A + B - 2 * C))', {
            'A': X_norm[:, None],
            'B': X_norm[None, :],
            'C': np.dot(X, X.T),
            'g': scaled_gamma
        })
    return K


def compute_kernel_from_states(data_batch):
    """Computes d[i][j] = |<psi_i|psi_j>|^2 """
    stride = 10  # must be multiple of n_data and be small enough to not overflow memory.
    batch_dim = tf.gather(tf.shape(data_batch), 0)
    full_circuits = data_batch
    empty_symbols = tf.convert_to_tensor([], dtype=tf.dtypes.string)
    empty_values = tf.tile(tf.convert_to_tensor([[]]), [stride, 1])
    ref_others = tf.tile(tf.expand_dims(data_batch, [0]), [stride, 1])

    res = np.empty((batch_dim, batch_dim))
    for i in range(0, batch_dim, stride):
        print('Processing state overlaps: ', i, '/', batch_dim.numpy())

        interm = inner_product_op.inner_product(data_batch[i:i + stride],
                                                empty_symbols, empty_values,
                                                ref_others)
        res[i:i + stride] = (tf.math.abs(interm)**2).numpy()

    return res


def get_spectrum_for_quantum_space(circuit_batch, rdm_dataset):
    # S, V for projected quantum kernel (qpk)
    KC_qs = compute_kernel_matrix(rdm_dataset, 1.0)

    S_qpk, V_qpk = np.linalg.eigh(KC_qs)
    S_qpk = np.abs(S_qpk)

    # S, V for quantum kernel (qk)
    Kqkall = tf.py_function(compute_kernel_from_states,
                            inp=[circuit_batch],
                            Tout=tf.float32).numpy()

    S_qk, V_qk = np.linalg.eigh(Kqkall)
    S_qk = np.abs(S_qk)

    return S_qpk, V_qpk, S_qk, V_qk, Kqkall


def test_dimension(Spectrum):
    Spectrum = sorted(Spectrum)[::-1]
    dimen = 0
    for i in range(len(Spectrum)):
        dimen += sum(Spectrum[i:]) / (len(Spectrum) - i)
    return dimen


def test_geometry(Spectrum, V_rotation):
    all_gamma = [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0]

    gmin, best_lambda, best_gamma = 9999999.0, 0.001, 1.0

    for gamma in all_gamma:
        Kc = compute_kernel_matrix(tf.convert_to_tensor(x_init), gamma)
        S2, V2 = np.linalg.eigh(Kc)
        S2 = np.abs(S2)

        for lambd in [0.00001, 0.001, 0.005, 0.01, 0.025, 0.05, 0.1]:
            Scaling = np.matmul(np.diag(Spectrum ** 0.5), np.matmul(V_rotation.T, np.matmul(V2, \
            np.matmul(np.diag(S2 / (S2 + lambd) ** 2), np.matmul(V2.T, np.matmul(V_rotation, np.diag(Spectrum ** 0.5)))))))
            #print("Generalization", np.linalg.norm(Scaling, ord=2) ** 0.5)

            Scaling2 = np.matmul(np.diag(Spectrum ** 0.5), np.matmul(V_rotation.T, np.matmul(V2, \
            np.matmul(np.diag((lambd / (S2 + lambd)) ** 2), np.matmul(V2.T, np.matmul(V_rotation, np.diag(Spectrum ** 0.5)))))))
            #print("Approximation", np.linalg.norm(Scaling2, ord=2))

            if gmin > np.linalg.norm(Scaling, ord=2)**0.5 and np.linalg.norm(
                    Scaling2, ord=2) < 0.002:
                gmin = np.linalg.norm(Scaling, ord=2)**0.5
                best_lambda = lambd
                best_gamma = gamma

    return gmin, best_lambda, best_gamma


# Prepare stilted dataset.
# S, V for kernel that can learn
def get_stilted_dataset(S, V, best_gamma, best_lambda):
    # S2, V2 for classical kernel
    Kc = compute_kernel_matrix(tf.convert_to_tensor(x_init), best_gamma)

    S2, V2 = np.linalg.eigh(Kc)
    S2 = np.abs(S2)

    # Construct an engineered dataset
    Scaling = np.matmul(np.diag(S ** 0.5), np.matmul(V.T, np.matmul(V2, \
    np.matmul(np.diag(S2 / (S2 + best_lambda) ** 2), np.matmul(V2.T, np.matmul(V, np.diag(S ** 0.5)))))))

    _, vecs = eigs(Scaling, k=1)
    print("Double-check hardness",
          np.matmul(vecs.T, np.matmul(Scaling, vecs))**0.5,
          np.matmul(vecs.T, vecs))
    y_label = np.real(np.matmul(V, np.matmul(np.diag(S**0.5), vecs)))
    y_label = np.array([1 if y > np.median(y_label) else -1 for y in y_label])
    y_label = np.array(
        [y if np.random.uniform() < 0.9 else -y for y in y_label])

    return y_label


def run_training_regression(x_init, x_new_init, K_qk, y_label, n_train):
    train_idx = np.arange(x_init.shape[0])
    np.random.shuffle(train_idx)
    train_idx = train_idx[:n_train]
    test_fold = np.array([0 for i in range(x_init.shape[0])])
    test_fold[train_idx] = -1
    ps = PredefinedSplit(test_fold)

    ret = {}
    # Try Feedforward neural network
    parameters = {
        'hidden_layer_sizes': [[10], [25], [50], [75], [100], [150], [200]]
    }
    mlp = MLPRegressor()
    clf = GridSearchCV(mlp,
                       parameters,
                       n_jobs=-1,
                       scoring='neg_mean_absolute_error',
                       cv=ps)
    clf.fit(x_init, y_label)

    best_index = np.argmin(clf.cv_results_['rank_test_score'])
    ret['NN'] = {
        'std': clf.cv_results_['std_test_score'][best_index],
        'mean': clf.cv_results_['mean_test_score'][best_index]
    }

    # Try regular SVM
    parameters = {
        'kernel': ['rbf', 'linear'],
        'C': [
            0.006, 0.015, 0.03, 0.0625, 0.125, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0,
            16.0, 32.0, 64.0, 128.0, 256, 512, 1024
        ],
        'gamma':
            np.array([0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 20.0]) /
            (x_init.shape[0] * x_init.var())
    }
    svc = SVR()
    clf = GridSearchCV(svc,
                       parameters,
                       n_jobs=-1,
                       scoring='neg_mean_absolute_error',
                       cv=ps)
    clf.fit(x_init, y_label)

    best_index = np.argmin(clf.cv_results_['rank_test_score'])
    ret['SVC'] = {
        'std': clf.cv_results_['std_test_score'][best_index],
        'mean': clf.cv_results_['mean_test_score'][best_index]
    }

    # Try RF.
    parameters2 = {
        'max_depth': [2, 3, 4, 5],
        'n_estimators': [25, 50, 100, 200, 500]
    }
    rf = RandomForestRegressor()
    clf2 = GridSearchCV(rf,
                        parameters2,
                        n_jobs=-1,
                        scoring='neg_mean_absolute_error',
                        cv=ps)
    clf2.fit(x_init, y_label)

    best_index = np.argmin(clf2.cv_results_['rank_test_score'])
    ret['RF'] = {
        'std': clf2.cv_results_['std_test_score'][best_index],
        'mean': clf2.cv_results_['mean_test_score'][best_index]
    }

    # Try GBT.
    parameters3 = {
        'max_depth': [2, 3, 4, 5],
        'n_estimators': [25, 50, 100, 200, 500]
    }
    gb = GradientBoostingRegressor()
    clf3 = GridSearchCV(gb,
                        parameters3,
                        n_jobs=-1,
                        scoring='neg_mean_absolute_error',
                        cv=ps)
    clf3.fit(x_init, y_label)

    best_index = np.argmin(clf3.cv_results_['rank_test_score'])
    ret['GBT'] = {
        'std': clf3.cv_results_['std_test_score'][best_index],
        'mean': clf3.cv_results_['mean_test_score'][best_index]
    }

    # Try ADA.
    parameters4 = {'n_estimators': [25, 50, 100, 200, 500]}
    ad = AdaBoostRegressor()
    clf4 = GridSearchCV(ad,
                        parameters4,
                        n_jobs=-1,
                        scoring='neg_mean_absolute_error',
                        cv=ps)
    clf4.fit(x_init, y_label)

    best_index = np.argmin(clf4.cv_results_['rank_test_score'])
    ret['ADA'] = {
        'std': clf4.cv_results_['std_test_score'][best_index],
        'mean': clf4.cv_results_['mean_test_score'][best_index]
    }

    # Try projected quantum SVM.
    fixed_gammas = np.array([0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 20.0
                            ]) / (x_new_init.shape[0] * x_new_init.var())
    parameters_f = {
        'kernel': ['rbf'],
        'C': [
            0.006, 0.015, 0.03, 0.0625, 0.125, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0,
            16.0, 32.0, 64.0, 128.0, 256, 512, 1024
        ],
        'gamma': fixed_gammas.tolist() + ['auto']
    }
    svc_f = SVR()
    clf_f = GridSearchCV(svc_f,
                         parameters_f,
                         n_jobs=-1,
                         scoring='neg_mean_absolute_error',
                         cv=ps)
    clf_f.fit(x_new_init, y_label)

    best_index = np.argmin(clf_f.cv_results_['rank_test_score'])
    ret['QPK'] = {
        'std': clf_f.cv_results_['std_test_score'][best_index],
        'mean': clf_f.cv_results_['mean_test_score'][best_index]
    }

    # Try quantum SVM.
    parameters_qk = {
        'kernel': ['precomputed'],
        'C': [
            0.006, 0.015, 0.03, 0.0625, 0.125, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0,
            16.0, 32.0, 64.0, 128.0, 256, 512, 1024
        ]
    }
    svc_qk = SVR()
    clf_qk = GridSearchCV(svc_qk,
                          parameters_qk,
                          n_jobs=-1,
                          scoring='neg_mean_absolute_error',
                          cv=ps)
    clf_qk.fit(K_qk, y_label)

    best_index = np.argmin(clf_qk.cv_results_['rank_test_score'])
    ret['QK'] = {
        'std': clf_qk.cv_results_['std_test_score'][best_index],
        'mean': clf_qk.cv_results_['mean_test_score'][best_index]
    }

    return ret


def run_training_classification(x_init, x_new_init, K_qk, y_label, n_train):
    train_idx = np.arange(x_init.shape[0])
    np.random.shuffle(train_idx)
    train_idx = train_idx[:n_train]
    test_fold = np.array([0 for i in range(x_init.shape[0])])
    test_fold[train_idx] = -1
    ps = PredefinedSplit(test_fold)

    ret = {}
    # Try Feedforward neural network
    parameters = {
        'hidden_layer_sizes': [[10], [25], [50], [75], [100], [150], [200]]
    }
    mlp = MLPClassifier()
    clf = GridSearchCV(mlp, parameters, n_jobs=-1, scoring='accuracy', cv=ps)
    clf.fit(x_init, y_label)

    best_index = np.argmin(clf.cv_results_['rank_test_score'])
    ret['NN'] = {
        'std': clf.cv_results_['std_test_score'][best_index],
        'mean': clf.cv_results_['mean_test_score'][best_index]
    }

    # Try regular SVM
    parameters = {
        'kernel': ['rbf', 'linear'],
        'C': [
            0.006, 0.015, 0.03, 0.0625, 0.125, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0,
            16.0, 32.0, 64.0, 128.0, 256, 512, 1024
        ],
        'gamma':
            np.array([0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 20.0]) /
            (x_init.shape[0] * x_init.var())
    }
    svc = SVC()
    clf = GridSearchCV(svc, parameters, n_jobs=-1, scoring='accuracy', cv=ps)
    clf.fit(x_init, y_label)

    best_index = np.argmin(clf.cv_results_['rank_test_score'])
    ret['SVC'] = {
        'std': clf.cv_results_['std_test_score'][best_index],
        'mean': clf.cv_results_['mean_test_score'][best_index]
    }

    # Try RF.
    parameters2 = {
        'max_depth': [2, 3, 4, 5],
        'n_estimators': [25, 50, 100, 200, 500]
    }
    rf = RandomForestClassifier()
    clf2 = GridSearchCV(rf, parameters2, n_jobs=-1, scoring='accuracy', cv=ps)
    clf2.fit(x_init, y_label)

    best_index = np.argmin(clf2.cv_results_['rank_test_score'])
    ret['RF'] = {
        'std': clf2.cv_results_['std_test_score'][best_index],
        'mean': clf2.cv_results_['mean_test_score'][best_index]
    }

    # Try GBT.
    parameters3 = {
        'max_depth': [2, 3, 4, 5],
        'n_estimators': [25, 50, 100, 200, 500]
    }
    gb = GradientBoostingClassifier()
    clf3 = GridSearchCV(gb, parameters3, n_jobs=-1, scoring='accuracy', cv=ps)
    clf3.fit(x_init, y_label)

    best_index = np.argmin(clf3.cv_results_['rank_test_score'])
    ret['GBT'] = {
        'std': clf3.cv_results_['std_test_score'][best_index],
        'mean': clf3.cv_results_['mean_test_score'][best_index]
    }

    # Try ADA.
    parameters4 = {'n_estimators': [25, 50, 100, 200, 500]}
    ad = AdaBoostClassifier()
    clf4 = GridSearchCV(ad, parameters4, n_jobs=-1, scoring='accuracy', cv=ps)
    clf4.fit(x_init, y_label)

    best_index = np.argmin(clf4.cv_results_['rank_test_score'])
    ret['ADA'] = {
        'std': clf4.cv_results_['std_test_score'][best_index],
        'mean': clf4.cv_results_['mean_test_score'][best_index]
    }

    # Try projected quantum SVM.
    fixed_gammas = np.array([0.25, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 20.0
                            ]) / (x_new_init.shape[0] * x_new_init.var())
    parameters_f = {
        'kernel': ['rbf'],
        'C': [
            0.006, 0.015, 0.03, 0.0625, 0.125, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0,
            16.0, 32.0, 64.0, 128.0, 256, 512, 1024
        ],
        'gamma': fixed_gammas.tolist() + ['auto']
    }
    svc_f = SVC()
    clf_f = GridSearchCV(svc_f,
                         parameters_f,
                         n_jobs=-1,
                         scoring='accuracy',
                         cv=ps)
    clf_f.fit(x_new_init, y_label)

    best_index = np.argmin(clf_f.cv_results_['rank_test_score'])
    ret['QPK'] = {
        'std': clf_f.cv_results_['std_test_score'][best_index],
        'mean': clf_f.cv_results_['mean_test_score'][best_index]
    }

    # Try quantum SVM.
    parameters_qk = {
        'kernel': ['precomputed'],
        'C': [
            0.006, 0.015, 0.03, 0.0625, 0.125, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0,
            16.0, 32.0, 64.0, 128.0, 256, 512, 1024
        ]
    }
    svc_qk = SVC()
    clf_qk = GridSearchCV(svc_qk,
                          parameters_qk,
                          n_jobs=-1,
                          scoring='accuracy',
                          cv=ps)
    clf_qk.fit(K_qk, y_label)

    best_index = np.argmin(clf_qk.cv_results_['rank_test_score'])
    ret['QK'] = {
        'std': clf_qk.cv_results_['std_test_score'][best_index],
        'mean': clf_qk.cv_results_['mean_test_score'][best_index]
    }

    return ret


# Data is now stored in :
# x_init => training data for regular models.
# x_new_init => training data for quantum projected kernel method.
# y_label => training labels for both datasets.

# Number of datapoints to generate.
n_points = 800
n_trotter = 20

all_results = []
sizes = [22
        ]  #[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
all_n_train = [100, 600]  #[25, 50, 100, 200, 300, 400, 600, 800]

for i in sizes:
    for embedding_option in [0, 1, 2]:
        for seed in range(10):
            t = time.time()
            print('Starting size:', i)
            # Number of qubits to use.
            qubits = cirq.GridQubit.rect(1, i)

            # Initial classical data source.
            #x_init = np.random.normal(0.0, 1.0, size=(n_points, len(qubits) - 1))
            x_init, y_init = generate_fashion_mnist(len(qubits) - 1, n_points)

            circuits_tensor, x_new_init, y_QNN = get_rdm_dataset(
                qubits, x_init, trotter=n_trotter, option=embedding_option)
            print("Obtained RDM")

            S_qpk, V_qpk, S_qk, V_qk, K_qk = get_spectrum_for_quantum_space(
                circuits_tensor, x_new_init)
            print("Obtained spectrum and rotation")

            d_qpk = test_dimension(S_qpk)
            d_qk = test_dimension(S_qk)
            print("Dimension (projected quantum kernel)", d_qpk)
            print("Dimension (quantum kernel)", d_qk)

            g_qpk, best_lambda, best_gamma = test_geometry(S_qpk, V_qpk)
            g_qk, best_lambda_qk, best_gamma_qk = test_geometry(S_qk, V_qk)
            print("Geometry (projected quantum kernel)", g_qpk)
            print("Geometry (quantum kernel)", g_qk)

            y_label_qpk = get_stilted_dataset(S_qpk, V_qpk, best_gamma,
                                              best_lambda)
            y_label_qk = get_stilted_dataset(S_qk, V_qk, best_gamma_qk,
                                             best_lambda_qk)
            print("Obtained y_label")

            # Result 1 = Original data
            fix_system_size_results = []
            for n_train in all_n_train:
                plotting_results = run_training_classification(
                    x_init, x_new_init.numpy(), K_qk, y_init, n_train)
                fix_system_size_results.append((n_train, plotting_results))
            all_results.append((i, embedding_option, seed, 101, d_qpk, d_qk,
                                g_qpk, g_qk, fix_system_size_results))

            # Result 2 = Arbitrary QNN data
            fix_system_size_results = []
            for n_train in all_n_train:
                plotting_results = run_training_regression(
                    x_init, x_new_init.numpy(), K_qk, y_QNN, n_train)
                fix_system_size_results.append((n_train, plotting_results))
            all_results.append((i, embedding_option, seed, 102, d_qpk, d_qk,
                                g_qpk, g_qk, fix_system_size_results))

            # Result 3 = Engineered data for QPK
            fix_system_size_results = []
            for n_train in all_n_train:
                plotting_results = run_training_classification(
                    x_init, x_new_init.numpy(), K_qk, y_label_qpk, n_train)
                fix_system_size_results.append((n_train, plotting_results))
            all_results.append((i, embedding_option, seed, 103, d_qpk, d_qk,
                                g_qpk, g_qk, fix_system_size_results))

            # Result 4 = Engineered data for QK
            fix_system_size_results = []
            for n_train in all_n_train:
                plotting_results = run_training_classification(
                    x_init, x_new_init.numpy(), K_qk, y_label_qk, n_train)
                fix_system_size_results.append((n_train, plotting_results))
            all_results.append((i, embedding_option, seed, 104, d_qpk, d_qk,
                                g_qpk, g_qk, fix_system_size_results))

            print('Finished size', i, 't=', time.time() - t, '(s)')
            pickle.dump(all_results, open("all_results.pkl", "wb"))
